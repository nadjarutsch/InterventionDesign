{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing DAG generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "from estimators import to_z, to_b, reinforce, relax, ExactGradientEstimator\n",
    "from distributions import PlackettLuce\n",
    "from critics import REBARCritic, RELAXCritic\n",
    "from utils import make_permutation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plackett-Luce distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.FloatTensor([10,10,-10,-10])\n",
    "dist = PlackettLuce(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [0, 1, 3, 2],\n",
       "        [1, 0, 3, 2],\n",
       "        [1, 0, 2, 3],\n",
       "        [0, 1, 2, 3],\n",
       "        [0, 1, 3, 2],\n",
       "        [0, 1, 3, 2],\n",
       "        [1, 0, 3, 2],\n",
       "        [0, 1, 2, 3],\n",
       "        [1, 0, 3, 2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sample(num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1.3863,  -1.3863, -42.0794])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(samples=torch.LongTensor([[0,1,2,3], [1,0,3,2], [3,2,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target [0 1 5 2 4 3]\n",
      "P_target tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "NUM_VARS = 6\n",
    "DETERMINISTIC_TARGET = True\n",
    "\n",
    "if DETERMINISTIC_TARGET:\n",
    "    target = np.random.permutation(NUM_VARS)\n",
    "    P_target = torch.zeros(NUM_VARS, NUM_VARS, dtype=torch.float32)\n",
    "    P_target[torch.arange(NUM_VARS), target] = 1.0\n",
    "\n",
    "    print(\"Target\", target)\n",
    "    print(\"P_target\", P_target)\n",
    "\n",
    "    loss_func = lambda P: torch.norm(P - P_target, p=2)\n",
    "else:\n",
    "    P_target = torch.rand(NUM_VARS, NUM_VARS)\n",
    "    P_target = P_target / P_target.sum(dim=-1, keepdims=True)\n",
    "    P_target = torch.distributions.utils.clamp_probs(P_target)\n",
    "    print(\"Target probs\", P_target)\n",
    "    P_target = P_target.log()\n",
    "    \n",
    "    loss_func = lambda P: -(P * P_target).sum() # NLL Loss\n",
    "    \n",
    "    min_loss = 10000\n",
    "    targets = None\n",
    "    for _ in range(5000):\n",
    "        t = np.random.permutation(NUM_VARS)\n",
    "        P = torch.zeros(NUM_VARS, NUM_VARS, dtype=torch.float32)\n",
    "        P[torch.arange(NUM_VARS), t] = 1.0\n",
    "        loss = loss_func(P).item()\n",
    "        if loss < min_loss:\n",
    "            min_loss, targets = loss, t\n",
    "    \n",
    "    print(\"Minimum loss found:\", min_loss)\n",
    "    print(\"Targets:\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_theta = nn.Parameter(torch.zeros(NUM_VARS) - torch.arange(NUM_VARS, dtype=torch.float32) / NUM_VARS + 0.5)\n",
    "critic = RELAXCritic(loss_func, NUM_VARS, 32)\n",
    "# critic = REBARCritic(loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(list(critic.parameters()) + [log_theta], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1091912a59154d76b41e38668d7d0adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.464101552963257\n",
      "Loss 3.464101552963257\n",
      "Loss 3.1622776985168457\n",
      "Loss 3.464101552963257\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.464101552963257\n",
      "Loss 3.464101552963257\n",
      "Loss 3.464101552963257\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.464101552963257\n",
      "Loss 3.464101552963257\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 3.464101552963257\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 3.1622776985168457\n",
      "Loss 3.464101552963257\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.464101552963257\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.464101552963257\n",
      "Loss 2.0\n",
      "Loss 3.464101552963257\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.4494898319244385\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 3.464101552963257\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 3.464101552963257\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 3.464101552963257\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 3.1622776985168457\n",
      "Loss 3.464101552963257\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 3.464101552963257\n",
      "Loss 3.1622776985168457\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 3.1622776985168457\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.4494898319244385\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.8284270763397217\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 2.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "Loss 0.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-d04fc4472f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mf_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_permutation_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0md_log_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0md_log_theta\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlog_theta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_log_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Causality/CausalGraphDiscovery/DAG_generation/estimators.py\u001b[0m in \u001b[0;36mrelax\u001b[0;34m(fb, b, logits, z, c, v, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reattach_z_to_new_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mz_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_z_tilde\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mc_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mc_z_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_tilde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Causality/CausalGraphDiscovery/DAG_generation/critics.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuralsortsoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-51d36e30e299>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(P)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P_target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mP_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mP_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_VARS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_VARS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/_VF.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVFModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5000)):\n",
    "    optim.zero_grad()\n",
    "    u = torch.distributions.utils.clamp_probs(torch.rand_like(log_theta)) # * 0.0 + 0.5\n",
    "    v = torch.distributions.utils.clamp_probs(torch.rand_like(log_theta)) # * 0.0 + 0.5\n",
    "    z = to_z(log_theta, u)\n",
    "    b = to_b(z)\n",
    "    f_b = loss_func(make_permutation_matrix(b))\n",
    "    d_log_theta = relax(fb=f_b, b=b, logits=log_theta, z=z, c=critic, v=v)\n",
    "    (d_log_theta ** 2).sum().backward()\n",
    "    log_theta.backward(d_log_theta)\n",
    "    optim.step()\n",
    "    print(\"Loss\", f_b.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 5, 4, 3])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(-log_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.6473,  0.4222,  0.1056, -0.3461, -0.2975,  0.0303],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6084e-06, -1.3169e-06, -1.3430e-06,  3.4524e-06,  8.3074e-07,\n",
       "        -2.0955e-08])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_theta.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinkhorn test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn(X, L, temp=1.):\n",
    "    \"\"\" Sinkhorn operator \"\"\"\n",
    "    if L==0:\n",
    "        S = torch.exp(X/temp)\n",
    "    else:\n",
    "        S = sinkhorn(X, L-1, temp)\n",
    "        S = S / torch.sum(S, axis=1, keepdims=True) # row normalize\n",
    "        S = S / torch.sum(S, axis=0, keepdims=True) # column normalize\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2767, 0.2532, 0.1119, 0.3582],\n",
      "        [0.4534, 0.1689, 0.3331, 0.0446],\n",
      "        [0.2371, 0.1726, 0.0997, 0.4906],\n",
      "        [0.0327, 0.4053, 0.4554, 0.1066]])\n",
      "tensor([3, 0, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "S = sinkhorn(X=torch.randn(4, 4), L=20, temp=1.0)\n",
    "print(S)\n",
    "print(S.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
